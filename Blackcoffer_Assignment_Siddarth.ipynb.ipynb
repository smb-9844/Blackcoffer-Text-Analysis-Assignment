{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4ca9d23-aaf5-43dd-92e4-8bc6fca8d1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dell8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dell8\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK packages ensured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "for pkg in ['punkt', 'stopwords']:\n",
    "    try:\n",
    "        nltk.data.find(pkg)\n",
    "    except Exception:\n",
    "        nltk.download(pkg)\n",
    "\n",
    "print('NLTK packages ensured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8619c4e1-5bb5-4f49-868e-709f825284d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, math\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "import textstat\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "112f42ee-6faa-48fd-a32a-9bbd0faa02dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path('.')\n",
    "STOPWORDS_DIR = Path('StopWords')\n",
    "MASTER_DICT_DIR = Path('MasterDictionary')\n",
    "ARTICLES_DIR = Path('extracted_articles')\n",
    "ARTICLES_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5a201db-ebe3-4c35-a71c-39739be1d8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(stopwords_dir=STOPWORDS_DIR):\n",
    "    sw = set()\n",
    "    if not stopwords_dir.exists():\n",
    "        print('StopWords folder not found:', stopwords_dir)\n",
    "        return sw\n",
    "    for f in stopwords_dir.glob('*.txt'):\n",
    "        with open(f, encoding='utf-8', errors='ignore') as fh:\n",
    "            for line in fh:\n",
    "                w = line.strip()\n",
    "                if w:\n",
    "                    sw.add(w.lower())\n",
    "    # also add nltk stopwords\n",
    "    try:\n",
    "        sw.update([w.lower() for w in stopwords.words('english')])\n",
    "    except Exception:\n",
    "        pass\n",
    "    return sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1d977fc-74dc-4967-a3e9-b143d1d56298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentiment_dicts(master_dir=MASTER_DICT_DIR):\n",
    "    pos=set(); neg=set()\n",
    "    posf = master_dir / 'positive words.txt'\n",
    "    negf = master_dir / 'negative words.txt'\n",
    "    # tolerant naming\n",
    "    for f in master_dir.glob('*.txt'):\n",
    "        name = f.name.lower()\n",
    "        if 'positive' in name:\n",
    "            with open(f, encoding='utf-8', errors='ignore') as fh:\n",
    "                pos.update({ln.strip().lower() for ln in fh if ln.strip()})\n",
    "        if 'negative' in name or 'negetive' in name:\n",
    "            with open(f, encoding='utf-8', errors='ignore') as fh:\n",
    "                neg.update({ln.strip().lower() for ln in fh if ln.strip()})\n",
    "    return pos, neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c27a3876-17db-4a6b-8546-49ec04d07bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords loaded: 12797 pos words: 2006 neg words: 4783\n"
     ]
    }
   ],
   "source": [
    "STOPWORDS = load_stopwords()\n",
    "POS_DICT, NEG_DICT = load_sentiment_dicts()\n",
    "\n",
    "print('stopwords loaded:', len(STOPWORDS), 'pos words:', len(POS_DICT), 'neg words:', len(NEG_DICT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "027ae773-3683-421a-857d-3441b705621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_html(url, timeout=30):\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}\n",
    "    r = requests.get(url, headers=headers, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76a4edcc-1a4a-4e83-9414-0775dd202d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title_and_body(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    # Title\n",
    "    title = ''\n",
    "    if soup.title:\n",
    "        title = soup.title.get_text(strip=True)\n",
    "    # Try <article> tags\n",
    "    body_text = ''\n",
    "    articles = soup.find_all('article')\n",
    "    if articles:\n",
    "        paras = []\n",
    "        for a in articles:\n",
    "            paras += [p.get_text(' ', strip=True) for p in a.find_all('p')]\n",
    "        body_text = '\\n'.join([p for p in paras if p and len(p)>30])\n",
    "    # Fallbacks: look for common container ids/classes\n",
    "    if not body_text:\n",
    "        candidates = []\n",
    "        for key in ['article', 'main', 'content', 'post', 'story', 'post-body', 'entry-content']:\n",
    "            el = soup.find(attrs={'id': re.compile(key, re.I)}) or soup.find(attrs={'class': re.compile(key, re.I)})\n",
    "            if el:\n",
    "                candidates.append(el)\n",
    "        if candidates:\n",
    "            paras=[]\n",
    "            for c in candidates:\n",
    "                paras += [p.get_text(' ', strip=True) for p in c.find_all('p')]\n",
    "            body_text = '\\n'.join([p for p in paras if p and len(p)>30])\n",
    "    # Final fallback: all <p> longer than 40 chars\n",
    "    if not body_text:\n",
    "        paras = soup.find_all('p')\n",
    "        body_text = '\\n'.join([p.get_text(' ', strip=True) for p in paras if len(p.get_text(strip=True))>40])\n",
    "    body_text = re.sub(r'\\n{2,}', '\\n\\n', body_text).strip()\n",
    "    return title.strip(), body_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b974c73-8a4e-4584-a7ed-e015e223906a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping helper ready\n"
     ]
    }
   ],
   "source": [
    "def save_article_text(url_id, title, body, out_dir=ARTICLES_DIR):\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "    fname = out_dir / f\"{url_id}.txt\"\n",
    "    with open(fname, 'w', encoding='utf-8') as fh:\n",
    "        if title:\n",
    "            fh.write(title + '\\n\\n')\n",
    "        fh.write(body)\n",
    "    return fname\n",
    "\n",
    "print('Scraping helper ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27eae8ce-8194-445c-bee4-1e78304fd2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSONAL_PRONOUNS_RE = re.compile(r\"\\b(I|we|We|WE|us|Us|US|my|My|our|Our|ours|Ours|me|Me)\\b\")\n",
    "\n",
    "def clean_text_for_counting(text, stopwords_set=STOPWORDS):\n",
    "    # remove URLs, emails\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+',' ', text)\n",
    "    text = re.sub(r'\\S+@\\S+',' ', text)\n",
    "    # remove punctuation except keep word separators\n",
    "    text = re.sub(r\"[^A-Za-z0-9\\s]\", ' ', text)\n",
    "    tokens = [t.lower() for t in word_tokenize(text) if re.search('[A-Za-z0-9]', t)]\n",
    "    tokens_cleaned = [t for t in tokens if t not in stopwords_set]\n",
    "    return tokens_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c7a7c51-fd16-478e-a823-a06c6db720ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word):\n",
    "    w = word.lower()\n",
    "    w = re.sub(r'[^a-z]','', w)\n",
    "    if not w:\n",
    "        return 0\n",
    "    if len(w) <= 3:\n",
    "        return 1\n",
    "    vowels = 'aeiou'\n",
    "    count = 0\n",
    "    prev_v = False\n",
    "    for ch in w:\n",
    "        is_v = ch in vowels\n",
    "        if is_v and not prev_v:\n",
    "            count += 1\n",
    "        prev_v = is_v\n",
    "\n",
    "    if w.endswith('es') or w.endswith('ed'):\n",
    "        if count > 1:\n",
    "            count -= 1\n",
    "\n",
    "    if w.endswith('e') and not w.endswith('le') and count>1:\n",
    "        count -=1\n",
    "    return max(1, count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b917984-8f50-4d8b-a6f6-179bdf940c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and metric functions ready\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(text, pos_dict=POS_DICT, neg_dict=NEG_DICT, stopwords_set=STOPWORDS):\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokens = clean_text_for_counting(text, stopwords_set)\n",
    "    total_words = len(tokens)\n",
    "    total_sentences = len(sentences) if len(sentences)>0 else 1\n",
    "\n",
    "\n",
    "    pos_count = sum(1 for w in tokens if w in pos_dict)\n",
    "    neg_count = sum(1 for w in tokens if w in neg_dict)\n",
    "    neg_count_positive = neg_count  \n",
    "\n",
    "\n",
    "    polarity = 0.0\n",
    "    if (pos_count + neg_count) != 0:\n",
    "        polarity = (pos_count - neg_count) / ((pos_count + neg_count) + 1e-6)\n",
    "    subjectivity = 0.0\n",
    "    if total_words != 0:\n",
    "        subjectivity = (pos_count + neg_count) / (total_words + 1e-6)\n",
    "\n",
    "\n",
    "    avg_sentence_length = total_words / total_sentences if total_sentences else 0.0\n",
    "\n",
    "\n",
    "    syll_counts = [count_syllables(w) for w in tokens]\n",
    "    total_syllables = sum(syll_counts)\n",
    "    syll_per_word = total_syllables / total_words if total_words else 0.0\n",
    "\n",
    "\n",
    "    complex_mask = [1 for s in syll_counts if s > 2]\n",
    "    complex_word_count = sum(complex_mask)\n",
    "    pct_complex = (complex_word_count / total_words)*100 if total_words else 0.0\n",
    "\n",
    "\n",
    "    fog_index = 0.4 * (avg_sentence_length + pct_complex)\n",
    "\n",
    "\n",
    "    avg_words_per_sentence = avg_sentence_length\n",
    "\n",
    "\n",
    "    personal_pronouns = len(PERSONAL_PRONOUNS_RE.findall(text))\n",
    "\n",
    "    personal_pronouns -= len(re.findall(r'\\bUS\\b', text))\n",
    "    if personal_pronouns < 0:\n",
    "        personal_pronouns = 0\n",
    "\n",
    "\n",
    "    word_lengths = [len(re.sub(r'[^A-Za-z0-9]','', w)) for w in tokens if re.sub(r'[^A-Za-z0-9]','', w)]\n",
    "    avg_word_length = sum(word_lengths)/len(word_lengths) if word_lengths else 0.0\n",
    "\n",
    "\n",
    "    metrics = {\n",
    "        'POSITIVE SCORE': pos_count,\n",
    "        'NEGATIVE SCORE': neg_count_positive,\n",
    "        'POLARITY SCORE': round(polarity, 6),\n",
    "        'SUBJECTIVITY SCORE': round(subjectivity, 6),\n",
    "        'AVG SENTENCE LENGTH': round(avg_sentence_length, 6),\n",
    "        'PERCENTAGE OF COMPLEX WORDS': round(pct_complex, 6),\n",
    "        'FOG INDEX': round(fog_index, 6),\n",
    "        'AVG NUMBER OF WORDS PER SENTENCE': round(avg_words_per_sentence, 6),\n",
    "        'COMPLEX WORD COUNT': complex_word_count,\n",
    "        'WORD COUNT': total_words,\n",
    "        'SYLLABLE PER WORD': round(syll_per_word, 6),\n",
    "        'PERSONAL PRONOUNS': personal_pronouns,\n",
    "        'AVG WORD LENGTH': round(avg_word_length, 6)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "print('Cleaning and metric functions ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72af9b7e-0b58-4a34-a4a2-675e87c3680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = 'Input.xlsx'  \n",
    "OUTPUT_FILE = 'output_result.xlsx'\n",
    "ARTICLES_DIR = Path('extracted_articles')\n",
    "ARTICLES_DIR.mkdir(exist_ok=True)\n",
    "ERR_LOG = 'errors.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0ae6382-4bf8-4267-b2ca-88c9673b4367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 147/147 [15:55<00:00,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Output: output_result.xlsx\n",
      "Articles saved to: extracted_articles\n",
      "Errors logged to: errors.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not Path(INPUT_FILE).exists():\n",
    "    print('Input.xlsx not found in current folder. Place it here and re-run this cell.')\n",
    "else:\n",
    "    df = pd.read_excel(INPUT_FILE)\n",
    "    required_cols = ['URL_ID','URL']\n",
    "    if not all(c in df.columns for c in required_cols):\n",
    "        print('Input.xlsx must contain columns URL_ID and URL')\n",
    "    else:\n",
    "        rows = []\n",
    "        errors = []\n",
    "        for _, r in tqdm(df.iterrows(), total=len(df)):\n",
    "            url = str(r['URL']).strip()\n",
    "            url_id = str(r['URL_ID']).strip()\n",
    "            try:\n",
    "                html = fetch_html(url)\n",
    "                title, body = extract_title_and_body(html)\n",
    "                if not body or len(body) < 50:\n",
    "                    errors.append((url_id, url, 'ExtractionTooShort'))\n",
    "                save_article_text(url_id, title, body)\n",
    "                full_text = (title + '\\n\\n' + body).strip()\n",
    "                metrics = compute_metrics(full_text)\n",
    "                out_row = r.to_dict()\n",
    "                # append metrics in exact order\n",
    "                order = ['POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE',\n",
    "                         'AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX',\n",
    "                         'AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT',\n",
    "                         'SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH']\n",
    "                for k in order:\n",
    "                    out_row[k] = metrics.get(k, '')\n",
    "                rows.append(out_row)\n",
    "            except Exception as e:\n",
    "                errors.append((url_id, url, str(e)))\n",
    "                # create blank metric row (so output row count matches input)\n",
    "                out_row = r.to_dict()\n",
    "                for k in ['POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE',\n",
    "                          'AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX',\n",
    "                          'AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT',\n",
    "                          'SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH']:\n",
    "                    out_row[k] = ''\n",
    "                rows.append(out_row)\n",
    "\n",
    "        out_df = pd.DataFrame(rows)\n",
    "        # reorder columns: original input columns then metrics in order\n",
    "        in_cols = list(df.columns)\n",
    "        metric_cols = ['POSITIVE SCORE','NEGATIVE SCORE','POLARITY SCORE','SUBJECTIVITY SCORE',\n",
    "                       'AVG SENTENCE LENGTH','PERCENTAGE OF COMPLEX WORDS','FOG INDEX',\n",
    "                       'AVG NUMBER OF WORDS PER SENTENCE','COMPLEX WORD COUNT','WORD COUNT',\n",
    "                       'SYLLABLE PER WORD','PERSONAL PRONOUNS','AVG WORD LENGTH']\n",
    "        final_cols = in_cols + metric_cols\n",
    "        out_df = out_df[final_cols]\n",
    "        out_df.to_excel(OUTPUT_FILE, index=False)\n",
    "        with open(ERR_LOG, 'w', encoding='utf-8') as fh:\n",
    "            for e in errors:\n",
    "                fh.write('\\t'.join(map(str,e)) + '\\n')\n",
    "        print('Processing complete. Output:', OUTPUT_FILE)\n",
    "        print('Articles saved to:', ARTICLES_DIR)\n",
    "        print('Errors logged to:', ERR_LOG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd69ea1f-4b64-4c53-a154-4878b1666cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (TensorFlow)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
